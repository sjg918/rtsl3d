https://discuss.pytorch.org/t/numpy-lexsort-equivalent-in-pytorch/47850

import numpy as np
from torch.nn import functional as F
import torch
import cv2

def read_calib_file(filepath):
    with open(filepath) as f:
        lines = f.readlines()

    obj = lines[2].strip().split(' ')[1:]
    P2 = np.array(obj, dtype=np.float32)
    obj = lines[3].strip().split(' ')[1:]
    P3 = np.array(obj, dtype=np.float32)
    obj = lines[4].strip().split(' ')[1:]
    R0 = np.array(obj, dtype=np.float32)
    obj = lines[5].strip().split(' ')[1:]
    Tr_velo_to_cam = np.array(obj, dtype=np.float32)

    return {'P2': P2.reshape(3, 4),
            'P3': P3.reshape(3, 4),
            'R_rect': R0.reshape(3, 3),
            'Tr_velo2cam': Tr_velo_to_cam.reshape(3, 4)}

calibs = read_calib_file('C:\Datasets/KITTI_OD/calib/000008.txt')
img = cv2.imread('C:\Datasets/KITTI_OD/image_2/000008.png')
sgm = cv2.imread('C:\Datasets/KITTI_OD/image_sgm/000008.png')
# Projection matrix from rect camera coord to image2 coord
P2 = calibs['P2']
P2 = np.reshape(P2, [3, 4])
P2 = torch.from_numpy(P2).unsqueeze(0)

h, w, _ = img.shape
x_range = np.arange(h, dtype=np.float32)
y_range = np.arange(w, dtype=np.float32)
_, yy_grid  = np.meshgrid(y_range, x_range)

yy_grid = torch.from_numpy(yy_grid).unsqueeze(0)
fy =  P2[:, 1:2, 1:2] #[B, 1, 1]
cy =  P2[:, 1:2, 2:3] #[B, 1, 1]
Ty =  P2[:, 1:2, 3:4] #[B, 1, 1]

#disparity = fy * 0.54  * (yy_grid - cy) / (torch.abs(fy * 1.65 + Ty) + 1e-10)
#disparity = F.relu(disparity)

#disparity = disparity.squeeze(0).numpy().astype(np.uint8)
#sgm = sgm[:, :, 0]
#dff = sgm.astype(np.int32) - disparity
#mask = np.abs(dff) <= 2
##dff[mask] = 0
##dff =dff.astype(np.uint8)

##mask = np.logical_not(mask)
#sgm[mask] = 0

#cv2.imshow('gg1', disparity)
#cv2.imshow('gg2', sgm)
#cv2.waitKey(0)
y_base = torch.linspace(-1, 1, h).repeat(1, w, 1).transpose(1, 2)
h_mean = 1.535
y_shifts_base = F.relu(
             (yy_grid - cy)
         / (yy_grid.shape[1] * 0.5)) * 0.1 # [1, H, W]
print(y_shifts_base)
cv2.imshow('gg1', (y_shifts_base.squeeze(0).numpy() * 255).astype(np.uint8))
cv2.waitKey(0)
df=df
class myConv2D(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, activation, norm=True, bias=False):
        super().__init__()

        pad = (kernel_size - 1) // 2
        self.layer = nn.ModuleList()
        self.layer.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad, bias=bias))
        if norm:
            self.layer.append(nn.BatchNorm2d(out_channels))
        elif activation == 'mish':
            self.layer.append(nn.Mish())
        elif activation == 'leaky':
            self.layer.append(nn.LeakyReLU(0.1))
        elif activation == 'relu':
            self.layer.append(nn.ReLU())
        elif activation == 'linear':
            pass
        else:
            raise Exception("cehck activation fucn.")

    def forward(self, x):
        for l in self.layer:
            x = l(x)
        return x

class DispRefinementHead(nn.Module):
    """Some Information about DispRefinementHead"""
    def __init__(self, in_channels):
        super(LookGround, self).__init__()
        self.conv1 = myConv2D(in_channels + 1, in_channels//2, 3, 1, 'relu')
        self.conv2 = myConv2D(in_channels//2, in_channels, 3, 1, 'relu')
        self.conv3 = myConv2D(in_channels, in_channels//2, 3, 1, 'relu')
        self.conv4 = myConv2D(in_channels//2, in_channels, 3, 1, 'relu')
        self.conv5 = myConv2D(in_channels, in_channels//2, 3, 1, 'relu')
        self.conv6 = myConv2D(in_channels//2, in_channels, 3, 1, 'relu')
        self.conv7 = myConv2D(in_channels, 1, 1, 1, 'linear', norm=False, bias=True)


    def forward(self, feature, dispmap):
        dispmap = dispmap.to(torch.float32).unsqueeze(dim=1)
        feature = torch.cat((feature, dispmap), dim=1)
        x1 = self.conv1(feature)
        x2 = self.conv2(x1)
        x3 = self.conv3(x2)
        x4 = self.conv4(x3)
        x5 = self.conv5(x4)
        x6 = self.conv6(x5)
        x7 = self.conv7(x6)
        refinement = F.tanh(x7) * 10
        return dispmap + refinement


class LookGround(nn.Module):
    """Some Information about LookGround"""
    def __init__(self, input_features, baseline=0.54, relative_elevation=1.65):
        super(LookGround, self).__init__()
        self.offset_create = nn.Sequential(
            nn.Conv2d(input_features, 1, 3, padding=1),
            nn.Tanh(),
        )
        self.extract = nn.Conv2d(1 + input_features, input_features, 1)
        self.baseline = baseline
        self.relative_elevation = relative_elevation
        self.alpha = nn.Parameter(torch.tensor([0.0], dtype=torch.float32))

    def forward(self, feature, dispmap, P2):

        dispmap = dispmap.to(torch.float32).unsqueeze(dim=1)
        feature = torch.cat((feature, dispmap), dim=1)
        offset = self.offset_estimate(feature)
        offset = 0.1 * (0.2 * offset + 0.8 * offset.detach())

        x = inputs['features']
        P2 = inputs['P2']

        P2 = P2.clone()
        P2[:, 0:2] /= 16.0 # downsample to 16.0

        disp = self.disp_create(x)
        disp = 0.1 * (0.05 * disp + 0.95 * disp.detach())

        batch_size, _, height, width = x.size()

        # Original coordinates of pixels
        x_base = torch.linspace(-1, 1, width).repeat(batch_size,
                    height, 1).type_as(x)
        y_base = torch.linspace(-1, 1, height).repeat(batch_size,
                    width, 1).transpose(1, 2).type_as(x)

        # Apply shift in Y direction
        h_mean = 1.535
        y_shifts_base = F.relu(
            h_mean * (yy_grid - cy) / (2 * (self.relative_elevation - 0.5 * h_mean))
        ) / (yy_grid.shape[1] * 0.5) # [1, H, W]
        y_shifts = y_shifts_base + disp[:, 0, :, :]  # Disparity is passed in NCHW format with 1 channel
        flow_field = torch.stack((x_base, y_base + y_shifts), dim=3)

        # In grid_sample coordinates are assumed to be between -1 and 1
        output = F.grid_sample(features, flow_field, mode='bilinear',
                               padding_mode='border', align_corners=True)

        return F.relu(x + self.extract(output) * self.alpha)
